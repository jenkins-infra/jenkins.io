---
layout: section
title: Using Docker with Pipeline
---
ifdef::backend-html5[]
:description:
:author:
:email: jenkinsci-docs@googlegroups.com
:sectanchors:
:toc:
ifdef::env-github[:imagesdir: ../resources]
ifndef::env-github[:imagesdir: ../../resources]
:hide-uri-scheme:
endif::[]

= Using Docker with Pipeline

Many organizations use link:https://www.docker.com[Docker] to unify their build
and test environments across machines, and to provide an efficient mechanism
for deploying applications. Starting with Pipeline versions 2.5 and higher,
Pipeline has built-in support for interacting with Docker from within a
`Jenkinsfile`.

While this section will cover the basics of utilizing Docker from with a
`Jenkinsfile`, it will not cover the fundamentals of Docker, which can be read
about in the
link:https://docs.docker.com/get-started/[Docker Getting Started Guide].


[[execution-environment]]
== Customizing the execution environment

Pipeline is designed to easily use
link:https://docs.docker.com/[Docker]
images as the execution environment for a single
link:../../glossary/#stage[Stage]
or the entire Pipeline. Meaning that a user can define the tools required for
their Pipeline, without having to manually configure agents.
Practically any tool which can be
link:https://hub.docker.com[packaged in a Docker container].
can be used with ease by making only minor edits to a `Jenkinsfile`.

[pipeline]
----
// Declarative //
pipeline {
    agent {
        docker { image 'node:7-alpine' }
    }
    stages {
        stage('Test') {
            steps {
                sh 'node --version'
            }
        }
    }
}
// Script //
node {
    /* Requires the Docker Pipeline plugin to be installed */
    docker.image('node:7-alpine').inside {
        stage('Test') {
            sh 'node --version'
        }
    }
}
----

When the Pipeline executes, Jenkins will automatically start the specified
container and execute the defined steps within it:

[source]
----
[Pipeline] stage
[Pipeline] { (Test)
[Pipeline] sh
[guided-tour] Running shell script
+ node --version
v7.4.0
[Pipeline] }
[Pipeline] // stage
[Pipeline] }
----

=== Caching data for containers

Many build tools will download external dependencies and cache them locally for
future re-use. Since containers are initially created with "clean" file
systems, this can result in slower Pipelines, as they may not take advantage of
on-disk caches between subsequent Pipeline runs.

Pipeline supports adding custom arguments which are passed
to Docker, allowing users to specify custom
link:https://docs.docker.com/engine/tutorials/dockervolumes/[Docker Volumes]
to mount, which can be used for caching data on the
link:../../glossary/#agent[agent]
between Pipeline runs. The following example will cache `~/.m2` between
Pipeline runs utilizing the
link:https://hub.docker.com/_/maven/[`maven` container],
 thereby avoiding the need to re-download dependencies for subsequent runs of
 the Pipeline.

[pipeline]
----
// Declarative //
pipeline {
    agent {
        docker {
            image 'maven:3-alpine'
            args '-v $HOME/.m2:/root/.m2'
        }
    }
    stages {
        stage('Build') {
            steps {
                sh 'mvn -B'
            }
        }
    }
}
// Script //
node {
    /* Requires the Docker Pipeline plugin to be installed */
    docker.image('maven:3-alpine').inside('-v $HOME/.m2:/root/.m2') {
        stage('Build') {
            sh 'mvn -B'
        }
    }
}
----



=== Using multiple containers

It has become increasingly common for code bases to rely on
multiple, different, technologies. For example, a repository might have both a
Java-based back-end API implementation _and_ a JavaScript-based front-end
implementation. Combining Docker and Pipeline allows a `Jenkinsfile` to use
*multiple* types of technologies by combining the `agent {}` directive, with
different stages.

[pipeline]
----
// Declarative //
pipeline {
    agent none
    stages {
        stage('Back-end') {
            agent {
                docker { image 'maven:3-alpine' }
            }
            steps {
                sh 'mvn --version'
            }
        }
        stage('Front-end') {
            agent {
                docker { image 'node:7-alpine' }
            }
            steps {
                sh 'node --version'
            }
        }
    }
}
// Script //
node {
    /* Requires the Docker Pipeline plugin to be installed */

    stage('Back-end') {
        docker.image('maven:3-alpine').inside {
            sh 'mvn --version'
        }
    }

    stage('Front-end') {
        docker.image('node:7-alpine').inside {
            sh 'node --version'
        }
    }
}
----

[[dockerfile]]
=== Using a Dockerfile

For projects which require a more customized execution environment, Pipeline
also supports building and running a container from a `Dockerfile` in the source
repository. In contrast to the <<execution-environment,previous approach>> of using
an "off-the-shelf" container, using the `agent { dockerfile true }` syntax will
build a new image from a `Dockerfile` rather than pulling one from
link:https://hub.docker.com[Docker Hub].

Re-using an example from above, with a more custom `Dockerfile`:

.Dockerfile
[source]
----
FROM node:7-alpine

RUN apk add -U subversion
----

By committing this to the root of the source repository, the `Jenkinsfile` can
be changed to build a container based on this `Dockerfile` and then run the
defined steps using that container:

[pipeline]
----
// Declarative //
pipeline {
    agent { dockerfile true }
    stages {
        stage('Test') {
            steps {
                sh 'node --version'
                sh 'svn --version'
            }
        }
    }
}
// Script //
----


The `agent { dockerfile true }` syntax supports a number of other options which
are described in more detail in the
link:../syntax#agent[Pipeline Syntax] section.

.Using a Dockerfile with Jenkins Pipeline
video::Pi2kJ2RJS50[youtube, width=852, height=480]


=== Specifying a Docker Label

By default, Pipeline assumes that _any_ configured
link:../../glossary/#agent[agent] is capable of running Docker-based Pipelines.
For Jenkins environments which have macOS, Windows, or other agents, which are
unable to run the Docker daemon, this default setting may be problematic.
Pipeline provides a global option in the **Manage Jenkins** page, and on
the
link:../../glossary/#folder[Folder]
level, for specifying which agents (by
link:../../glossary/#label[Label])
to use for running Docker-based Pipelines.

image::pipeline/configure-docker-label.png[Configuring the Pipeline Docker Label]

== Advanced Usage with Scripted Pipeline

=== Running "sidecar" containers

Using Docker in Pipeline can be an effective way to run a service on which the
build, or a set of tests, may rely. Similar to the
link:https://docs.microsoft.com/en-us/azure/architecture/patterns/sidecar[sidecar
pattern], Docker Pipeline can run one container "in the background", while
performing work in another. Utilizing this sidecar approach, a Pipeline can
have a "clean" container provisioned for each Pipeline run.

Consider a hypothetical integration test suite which relies on a local MySQL
database to be running. Using the `withRun` method, implemented in the
plugin:docker-workflow[Docker Pipeline] plugin's support for Scripted Pipeline,
a `Jenkinsfile` can run MySQL as a sidecar:

[source,groovy]
----
node {
    checkout scm
    /*
     * In order to communicate with the MySQL server, this Pipeline explicitly
     * maps the port (`3306`) to a known port on the host machine.
     */
    docker.image('mysql:5').withRun('-e "MYSQL_ROOT_PASSWORD=my-secret-pw" -p 3306:3306') { c ->
        /* Wait until mysql service is up */
        sh 'while ! mysqladmin ping -h0.0.0.0 --silent; do sleep 1; done'
        /* Run some tests which require MySQL */
        sh 'make check'
    }
}
----

This example can be taken further, utilizing two containers simultaneously.
One "sidecar" running MySQL, and another providing the <<execution-environment,
execution environment>>, by using the Docker
link:https://docs.docker.com/engine/userguide/networking/default_network/dockerlinks/[container links].

[source,groovy]
----
node {
    checkout scm
    docker.image('mysql:5').withRun('-e "MYSQL_ROOT_PASSWORD=my-secret-pw"') { c ->
        docker.image('mysql:5').inside("--link ${c.id}:db") {
            /* Wait until mysql service is up */
            sh 'while ! mysqladmin ping -hdb --silent; do sleep 1; done'
        }
        docker.image('centos:7').inside("--link ${c.id}:db") {
            /*
             * Run some tests which require MySQL, and assume that it is
             * available on the host name `db`
             */
            sh 'make check'
        }
    }
}
----

The above example uses the object exposed by `withRun`, which has the
running container's ID available via the `id` property. Using the container's
ID, the Pipeline can create a link by passing custom Docker arguments to the
`inside()` method.


The `id` property can also be useful for inspecting logs from a running Docker
container before the Pipeline exits:

[source,groovy]
----
sh "docker logs ${c.id}"
----


=== Building containers


In order to create a Docker image, the plugin:docker-workflow[Docker Pipeline]
plugin also provides a `build()` method for creating a new image, from a
`Dockerfile` in the repository, during a Pipeline run.

One major benefit of using the syntax `docker.build("my-image-name")` is that a
Scripted Pipeline can use the return value for subsequent Docker Pipeline
calls, for example:

[source,groovy]
----
node {
    checkout scm

    def customImage = docker.build("my-image:${env.BUILD_ID}")

    customImage.inside {
        sh 'make test'
    }
}
----


The return value can also be used to publish the Docker image to
link:https://hub.docker.com[Docker Hub],
or a <<custom-registry, custom Registry>>,
via the `push()` method, for example:

[source,groovy]
----
node {
    checkout scm
    def customImage = docker.build("my-image:${env.BUILD_ID}")
    customImage.push()
}
----

One common usage of image "tags" is to specify a `latest` tag for the most
recently, validated, version of a Docker image. The `push()` method accepts an
optional `tag` parameter, allowing the Pipeline to push the `customImage` with
different tags, for example:

[source,groovy]
----
node {
    checkout scm
    def customImage = docker.build("my-image:${env.BUILD_ID}")
    customImage.push()

    customImage.push('latest')
}
----

The `build()` method builds the `Dockerfile` in the current directory by 
default. This can be overridden by providing a directory path 
containing a `Dockerfile` as the second argument of the `build()` method, for example:

[source,groovy]
----
node {
    checkout scm
    def testImage = docker.build("test-image", "./dockerfiles/test") // <1>

    testImage.inside {
        sh 'make test'
    }
}
----
<1> Builds `test-image` from the Dockerfile found at `./dockerfiles/test/Dockerfile`.

It is possible to pass other arguments to 
link:https://docs.docker.com/engine/reference/commandline/build/[docker build]
by adding them to the second argument of the `build()` method.
When passing arguments this way, the last value in the that string must be 
the path to the docker file.

This example overrides the default `Dockerfile` by passing the `-f`
flag:

[source,groovy]
----
node {
    checkout scm
    def dockerfile = 'Dockerfile.test'
    def customImage = docker.build("my-image:${env.BUILD_ID}", "-f ${dockerfile} ./dockerfiles") // <1>
}
----
<1> Builds `my-image:${env.BUILD_ID}` from the Dockerfile found at `./dockerfiles/Dockerfile.test`. 

=== Using a remote Docker server

By default, the plugin:docker-workflow[Docker Pipeline] plugin will communicate
with a local Docker daemon, typically accessed through `/var/run/docker.sock`.


To select a non-default Docker server, such as with
link:https://docs.docker.com/swarm/[Docker Swarm],
the `withServer()` method should be used.

By passing a URI, and optionally the Credentials ID of a **Docker Server
Certificate Authentication** pre-configured in Jenkins, to the method with:


[source,groovy]
----
node {
    checkout scm

    docker.withServer('tcp://swarm.example.com:2376', 'swarm-certs') {
        docker.image('mysql:5').withRun('-p 3306:3306') {
            /* do things */
        }
    }
}
----

[CAUTION]
====
`inside()` and `build()` will not work properly with a Docker Swarm server out
of the box

For `inside()` to work, the Docker server and the Jenkins agent must use the
same filesystem, so that the workspace can be mounted.

Currently neither the Jenkins plugin nor the Docker CLI will automatically
detect the case that the server is running remotely; a typical symptom would be
errors from nested `sh` commands such as

[source]
----
cannot create /…@tmp/durable-…/pid: Directory nonexistent
----

When Jenkins detects that the agent is itself running inside a Docker
container, it will automatically pass the `--volumes-from` argument to the
`inside` container, ensuring that it can share a workspace with the agent.

Additionally some versions of Docker Swarm do not support custom Registries.
====




[[custom-registry]]
=== Using a custom registry

By default the plugin:docker-workflow[Docker Pipeline] integrates assumes the
default Docker Registry of
link:https://hub.docker.com[Docker Hub].

In order to use a custom Docker Registry, users of Scripted Pipeline can wrap
steps with the `withRegistry()` method, passing in the custom Registry URL, for
example:

[source, groovy]
----
node {
    checkout scm

    docker.withRegistry('https://registry.example.com') {

        docker.image('my-custom-image').inside {
            sh 'make test'
        }
    }
}
----

For a Docker Registry which requires authentication, add a "Username/Password"
Credentials item from the Jenkins home page and use the Credentials ID as a
second argument to `withRegistry()`:

[source, groovy]
----
node {
    checkout scm

    docker.withRegistry('https://registry.example.com', 'credentials-id') {

        def customImage = docker.build("my-image:${env.BUILD_ID}")

        /* Push the container to the custom Registry */
        customImage.push()
    }
}
----
