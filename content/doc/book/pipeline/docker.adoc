---
layout: section
title: Using Docker with Pipeline
---
ifdef::backend-html5[]
:description:
:author:
:sectanchors:
:toc:
ifdef::env-github[:imagesdir: ../resources]
ifndef::env-github[:imagesdir: ../../resources]
:hide-uri-scheme:
endif::[]

= Using Docker with Pipeline

Many organizations use link:https://www.docker.com[Docker] to unify their build and test environments across machines, and to provide an efficient mechanism for deploying applications.
Starting with Pipeline versions 2.5 and higher, Pipeline has built-in support for interacting with Docker from within a `Jenkinsfile`.

While this page covers the basics of utilizing Docker from within a `Jenkinsfile`, it will not cover the fundamentals of Docker, which you can refer to in the link:https://docs.docker.com/get-started/[Docker Getting Started Guide].


[[execution-environment]]
== Customizing the execution environment

Pipeline is designed to easily use link:https://docs.docker.com/[Docker] images as the execution environment for a single link:../../glossary/#stage[Stage] or the entire Pipeline.
Meaning that a user can define the tools required for their Pipeline, without having to manually configure agents.
Any tool that can be link:https://hub.docker.com[packaged in a Docker container] can be used with ease, by making only minor edits to a `Jenkinsfile`.

[pipeline]
----
// Declarative //
pipeline {
    agent {
        docker { image 'node:20.15.1-alpine3.20' }
    }
    stages {
        stage('Test') {
            steps {
                sh 'node --version'
            }
        }
    }
}
// Script //
node {
    /* Requires the Docker Pipeline plugin to be installed */
    docker.image('node:20.15.1-alpine3.20').inside {
        stage('Test') {
            sh 'node --version'
        }
    }
}
----

When the Pipeline executes, Jenkins will automatically start the specified container and execute the defined steps within:

[source]
----
[Pipeline] stage
[Pipeline] { (Test)
[Pipeline] sh
[guided-tour] Running shell script
+ node --version
v16.13.1
[Pipeline] }
[Pipeline] // stage
[Pipeline] }
----

=== Workspace synchronization

If it is important to keep the workspace synchronized with other stages, use `reuseNode true`.
Otherwise, a dockerized stage can be run on the same agent or any other agent, but in a temporary workspace.

By default, for a containerized stage, Jenkins:

* Picks an agent.
* Creates a new empty workspace.
* Clones pipeline code into it.
* Mounts this new workspace into the container.

If you have multiple Jenkins agents, your containerized stage can be started on any of them.

When `reuseNode` is set to `true`, no new workspace will be created, and the current workspace from the current agent will be mounted into the container. 
After this, the container will be started on the same node, so all of the data will be synchronized.

[pipeline]
----
// Declarative //
pipeline {
    agent any
    stages {
        stage('Build') {
            agent {
                docker {
                    image 'gradle:8.2.0-jdk17-alpine'
                    // Run the container on the node specified at the
                    // top-level of the Pipeline, in the same workspace,
                    // rather than on a new node entirely:
                    reuseNode true
                }
            }
            steps {
                sh 'gradle --version'
            }
        }
    }
}
// Script //
// Option "reuseNode true" currently unsupported in scripted pipeline
----


=== Caching data for containers

Many build tools will download external dependencies and cache them locally for future re-use.
Since containers are initially created with "clean" file systems, this can result in slower Pipelines, as they may not take advantage of on-disk caches between subsequent Pipeline runs.

Pipeline supports adding custom arguments that are passed to Docker, allowing users to specify custom link:https://docs.docker.com/engine/tutorials/dockervolumes/[Docker Volumes] to mount, which can be used for caching data on the link:../../glossary/#agent[agent] between Pipeline runs.
The following example will cache `~/.m2` between Pipeline runs utilizing the link:https://hub.docker.com/_/maven/[`maven` container], avoiding the need to re-download dependencies for subsequent Pipeline runs.

[pipeline]
----
// Declarative //
pipeline {
    agent {
        docker {
            image 'maven:3.9.3-eclipse-temurin-17'
            args '-v $HOME/.m2:/root/.m2'
        }
    }
    stages {
        stage('Build') {
            steps {
                sh 'mvn -B'
            }
        }
    }
}
// Script //
node {
    /* Requires the Docker Pipeline plugin to be installed */
    docker.image('maven:3.9.8-eclipse-temurin-21-alpine').inside('-v $HOME/.m2:/root/.m2') {
        stage('Build') {
            sh 'mvn -B'
        }
    }
}
----



=== Using multiple containers

It has become increasingly common for code bases to rely on multiple different technologies.
For example, a repository might have both a Java-based back-end API implementation _and_ a JavaScript-based front-end implementation.
Combining Docker and Pipeline allows a `Jenkinsfile` to use *multiple* types of technologies, by combining the `agent {}` directive with different stages.

[pipeline]
----
// Declarative //
pipeline {
    agent none
    stages {
        stage('Back-end') {
            agent {
                docker { image 'maven:3.9.8-eclipse-temurin-21-alpine' }
            }
            steps {
                sh 'mvn --version'
            }
        }
        stage('Front-end') {
            agent {
                docker { image 'node:20.15.1-alpine3.20' }
            }
            steps {
                sh 'node --version'
            }
        }
    }
}
// Script //
node {
    /* Requires the Docker Pipeline plugin to be installed */

    stage('Back-end') {
        docker.image('maven:3.9.8-eclipse-temurin-21-alpine').inside {
            sh 'mvn --version'
        }
    }

    stage('Front-end') {
        docker.image('node:20.15.1-alpine3.20').inside {
            sh 'node --version'
        }
    }
}
----

[[dockerfile]]
=== Using a Dockerfile

For projects requiring a more customized execution environment, Pipeline also supports building and running a container from a `Dockerfile` in the source repository.
In contrast to the <<execution-environment,previous approach>> of using an "off-the-shelf" container, using the `agent { dockerfile true }` syntax builds a new image from a `Dockerfile`, rather than pulling one from link:https://hub.docker.com[Docker Hub].

Reusing an example from above, with a more custom `Dockerfile`:

.Dockerfile
[source]
----
FROM node:20.15.1-alpine3.20

RUN apk add -U subversion
----

By committing this to the root of the source repository, the `Jenkinsfile` can be changed to build a container based on this `Dockerfile`, and then run the defined steps using that container:

[pipeline]
----
// Declarative //
pipeline {
    agent { dockerfile true }
    stages {
        stage('Test') {
            steps {
                sh 'node --version'
                sh 'svn --version'
            }
        }
    }
}
// Script //
----


The `agent { dockerfile true }` syntax supports a number of other options, which are described in more detail in the link:../syntax#agent[Pipeline Syntax] section.

.Using a Dockerfile with Jenkins Pipeline
video::Pi2kJ2RJS50[youtube, width=852, height=480]


=== Specifying a Docker Label

By default, Pipeline assumes that _any_ configured link:../../glossary/#agent[agent] is capable of running Docker-based Pipelines.
For Jenkins environments that have macOS, Windows, or other agents that are unable to run the Docker daemon, this default setting may be problematic.
Pipeline provides a global option on the *Manage Jenkins* page and on the link:../../glossary/#folder[Folder] level, for specifying which agents (by link:../../glossary/#label[Label]) to use for running Docker-based Pipelines.

image::pipeline/configure-docker-label.png[Configuring the Pipeline Docker Label]

=== Path setup for mac OS users

The `/usr/local/bin` directory is not included in the macOS `PATH` for Docker images by default.
If executables from `/usr/local/bin` need to be called from within Jenkins, the `PATH` needs to be extended to include `/usr/local/bin`.
Add a path node in the file "/usr/local/Cellar/jenkins-lts/XXX/homebrew.mxcl.jenkins-lts.plist" like this:

.Contents of homebrew.mxcl.jenkins-lts.plist
[source,xml]
----
<key>EnvironmentVariables</key>
<dict>
<key>PATH</key>
<string><!-- insert revised path here --></string>
</dict>
----

The revised `PATH` `string` should be a colon separated list of directories in the same format as the `PATH` environment variable and should include:

* `/usr/local/bin`
* `/usr/bin`
* `/bin`
* `/usr/sbin`
* `/sbin`
* `/Applications/Docker.app/Contents/Resources/bin/`
* `/Users/XXX/Library/Group\ Containers/group.com.docker/Applications/Docker.app/Contents/Resources/bin` (where `XXX` is replaced by your user name)

Now, restart jenkins using `brew services restart jenkins-lts`.

== Advanced Usage with Scripted Pipeline

=== Running "sidecar" containers

Using Docker in Pipeline is an effective way to run a service on which the build, or a set of tests, may rely.
Similar to the link:https://docs.microsoft.com/en-us/azure/architecture/patterns/sidecar[sidecar pattern], Docker Pipeline can run one container "in the background", while performing work in another.
Utilizing this sidecar approach, a Pipeline can have a "clean" container provisioned for each Pipeline run.

Consider a hypothetical integration test suite that relies on a local MySQL database to be running.
Using the `withRun` method, implemented in the plugin:docker-workflow[Docker Pipeline] plugin's support for Scripted Pipeline, a `Jenkinsfile` can run MySQL as a sidecar:

[source,groovy]
----
node {
    checkout scm
    /*
     * In order to communicate with the MySQL server, this Pipeline explicitly
     * maps the port (`3306`) to a known port on the host machine.
     */
    docker.image('mysql:8-oracle').withRun('-e "MYSQL_ROOT_PASSWORD=my-secret-pw"' +
                                           ' -p 3306:3306') { c ->
        /* Wait until mysql service is up */
        sh 'while ! mysqladmin ping -h0.0.0.0 --silent; do sleep 1; done'
        /* Run some tests which require MySQL */
        sh 'make check'
    }
}
----

This example can be taken further, utilizing two containers simultaneously.
One "sidecar" running MySQL, and another providing the <<execution-environment,execution environment>> by using the Docker link:https://docs.docker.com/engine/userguide/networking/default_network/dockerlinks/[container links].

[source,groovy]
----
node {
    checkout scm
    docker.image('mysql:8-oracle').withRun('-e "MYSQL_ROOT_PASSWORD=my-secret-pw"') { c ->
        docker.image('mysql:8-oracle').inside("--link ${c.id}:db") {
            /* Wait until mysql service is up */
            sh 'while ! mysqladmin ping -hdb --silent; do sleep 1; done'
        }
        docker.image('oraclelinux:9').inside("--link ${c.id}:db") {
            /*
             * Run some tests that require MySQL, and assume that it is
             * available on the host name `db`
             */
            sh 'make check'
        }
    }
}
----

The above example uses the object exposed by `withRun`, which has the running container's ID available via the `id` property.
Using the container's ID, the Pipeline can create a link by passing custom Docker arguments to the `inside()` method.


The `id` property can also be useful for inspecting logs from a running Docker container before the Pipeline exits:

[source,groovy]
----
sh "docker logs ${c.id}"
----


=== Building containers

In order to create a Docker image, the plugin:docker-workflow[Docker Pipeline] plugin also provides a `build()` method for creating a new image from a `Dockerfile` in the repository during a Pipeline run.

One major benefit of using the syntax `docker.build("my-image-name")` is that a Scripted Pipeline can use the return value for subsequent Docker Pipeline calls, for example:

[source,groovy]
----
node {
    checkout scm

    def customImage = docker.build("my-image:${env.BUILD_ID}")

    customImage.inside {
        sh 'make test'
    }
}
----


The return value can also be used to publish the Docker image to link:https://hub.docker.com[Docker Hub] or a <<custom-registry, custom Registry>>, via the `push()` method, for example:

[source,groovy]
----
node {
    checkout scm
    def customImage = docker.build("my-image:${env.BUILD_ID}")
    customImage.push()
}
----

One common usage of image "tags" is to specify a `latest` tag for the most recently validated version of a Docker image.
The `push()` method accepts an optional `tag` parameter, allowing the Pipeline to push the `customImage` with different tags, for example:

[source,groovy]
----
node {
    checkout scm
    def customImage = docker.build("my-image:${env.BUILD_ID}")
    customImage.push()

    customImage.push('latest')
}
----

The `build()` method builds the `Dockerfile` in the current directory by default.
This can be overridden by providing a directory path containing a `Dockerfile` as the second argument of the `build()` method, for example:

[source,groovy]
----
node {
    checkout scm
    def testImage = docker.build("test-image", "./dockerfiles/test") // <1>

    testImage.inside {
        sh 'make test'
    }
}
----
<1> Builds `test-image` from the Dockerfile found at `./dockerfiles/test/Dockerfile`.

It is possible to pass other arguments to link:https://docs.docker.com/engine/reference/commandline/build/[`docker build`] by adding them to the second argument of the `build()` method.
When passing arguments this way, the last value in the string must be the path to the docker file, and should end with the folder to use as the build context.

This example overrides the default `Dockerfile` by passing the `-f` flag:

[source,groovy]
----
node {
    checkout scm
    def dockerfile = 'Dockerfile.test'
    def customImage = docker.build("my-image:${env.BUILD_ID}",
                                   "-f ${dockerfile} ./dockerfiles") // <1>
}
----
<1> Builds `my-image:${env.BUILD_ID}` from the Dockerfile found at `./dockerfiles/Dockerfile.test`. 

=== Using a remote Docker server

By default, the plugin:docker-workflow[Docker Pipeline] plugin will communicate with a local Docker daemon, typically accessed through `/var/run/docker.sock`.


To select a non-default Docker server, such as with link:https://docs.docker.com/swarm/[Docker Swarm], use the `withServer()` method.

You can pass a URI, and optionally the Credentials ID of a *Docker Server Certificate Authentication* pre-configured in Jenkins, to the method with:


[source,groovy]
----
node {
    checkout scm

    docker.withServer('tcp://swarm.example.com:2376', 'swarm-certs') {
        docker.image('mysql:8-oracle').withRun('-p 3306:3306') {
            /* do things */
        }
    }
}
----

[CAUTION]
====
`inside()` and `build()` will not work properly with a Docker Swarm server out of the box.

For `inside()` to work, the Docker server and the Jenkins agent must use the same filesystem, so that the workspace can be mounted.

Currently, neither the Jenkins plugin nor the Docker CLI will automatically detect the case that the server is running remotely.
A typical symptom of this would be errors from nested `sh` commands such as:

[source]
----
cannot create /…@tmp/durable-…/pid: Directory nonexistent
----

When Jenkins detects that the agent is itself running inside a Docker container, it will automatically pass the `--volumes-from` argument to the `inside` container, ensuring that it can share a workspace with the agent.

Additionally, some versions of Docker Swarm do not support custom Registries.
====




[[custom-registry]]
=== Using a custom registry

By default, the plugin:docker-workflow[Docker Pipeline] plugin assumes the default Docker Registry of link:https://hub.docker.com[Docker Hub].

In order to use a custom Docker Registry, users of Scripted Pipeline can wrap steps with the `withRegistry()` method, passing in the custom Registry URL, for example:

[source, groovy]
----
node {
    checkout scm

    docker.withRegistry('https://registry.example.com') {

        docker.image('my-custom-image').inside {
            sh 'make test'
        }
    }
}
----

For a Docker Registry requiring authentication, add a "Username/Password" Credentials item from the Jenkins home page and use the Credentials ID as a second argument to `withRegistry()`:

[source, groovy]
----
node {
    checkout scm

    docker.withRegistry('https://registry.example.com', 'credentials-id') {

        def customImage = docker.build("my-image:${env.BUILD_ID}")

        /* Push the container to the custom Registry */
        customImage.push()
    }
}
----
