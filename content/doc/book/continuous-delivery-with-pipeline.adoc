:imagesdir: ../resources/

== Continuous Delivery with Jenkins Pipeline

=== Introduction

Continuous delivery allows organizations to deliver software with lower risk. The path to continuous delivery starts by modeling the software delivery pipeline used within the organization and then focusing on the automation of it all. Early, directed feedback, enabled by pipeline automation enables software delivery more quickly over traditional methods of delivery.

Jenkins is the Swiss army knife in the software delivery toolchain. Developers and operations (DevOps) personnel have different mindsets and use different tools to get their respective jobs done. Since Jenkins integrates with a huge variety of toolsets, it serves as the intersection point between development and operations teams.

Many organizations have been orchestrating pipelines with existing Jenkins plugins for several years. As their automation sophistication and their own Jenkins experience increases, organizations inevitably want to move beyond simple pipelines and create complex flows specific to their delivery process. 

These Jenkins users require a feature that treats complex pipelines as a first-class object, and so CloudBees engineers developed and contributed the new https://wiki.jenkins-ci.org/display/JENKINS/Workflow+Plugin[Jenkins Pipeline feature] to the Jenkins open source project. This Pipeline plugin was built with the community’s requirements for a flexible, extensible, and script-based pipeline in mind.

=== Prerequisites

Continuous delivery is a process - rather than a tool - and requires a mindset and culture that must percolate from the top-down within an organization. Once the organization has bought into the philosophy, the next and most difficult part is mapping the flow of software as it makes its way from development to production.

The root of such a pipeline will always be an orchestration tool like a Jenkins, but there are some key requirements that such an integral part of the pipeline must satisfy before it can be tasked with enterprise-critical processes:

* *Zero or low downtime disaster recovery*: A commit, just as a mythical hero, encounters harder and longer challenges as it makes its way down the pipeline. It is not unusual to see pipeline executions that last days. A hardware or a Jenkins failure on day six of a seven-day pipeline has serious consequences for on-time delivery of a product.

* *Audit runs and debug ability*: Build managers like to see the exact execution flow through the pipeline, so they can easily debug issues.

To ensure a tool can scale with an organization and suitably automate existing delivery pipelines without changing them, the tool should also support:

* *Complex pipelines*: Delivery pipelines are typically more complex than canonical examples (linear process: Dev->Test->Deploy, with a couple of operations at each stage). Build managers want constructs that help parallelize parts of the flow, run loops, perform retries and so forth. Stated differently, build managers want programming constructs to define pipelines.

* *Manual interventions*: Pipelines cross intra-organizational boundaries necessitating manual handoffs and interventions. Build managers seek capabilities such as being able to pause a pipeline for a human to intervene and make manual decisions.

The Pipeline plugin allows users to create such a pipeline through a new job type called Pipeline. The flow definition is captured in a Groovy script, thus adding control flow capabilities such as loops, forks and retries. Pipeline allows for stages with the option to set concurrencies, preventing multiple builds of the same pipeline from trying to access the same resource at the same time.

=== Concepts

.Pipeline Job Type

There is just one job to capture the entire software delivery pipeline in an organization. Of course, you can still connect two Pipeline job types together if you want. A Pipeline job type uses a Groovy-based DSL for job definitions. The DSL affords the advantage of defining jobs programmatically:


[source, width="350"]
node(‘linux’){
  git url: 'https://github.com/jglick/simple-maven-project-with-tests.git'
  def mvnHome = tool 'M3'
  env.PATH = "${mvnHome}/bin:${env.PATH}"
  sh 'mvn -B clean verify'
}

.Stages

Intra-organizational (or conceptual) boundaries are captured through a primitive called “stages.” A deployment pipeline consists of various stages, where each subsequent stage
builds on the previous one. The idea is to spend as few resources as possible early in the pipeline and find obvious issues, rather than spend a lot of computing resources for something that is ultimately discovered to be broken.

[[throttled-concurrent]]
.Throttled stage concurrency with Pipeline
image::ch13/stage-concurrency.png[scaledwidth=“90%”]

Consider a simple pipeline with three stages. A naive implementation of this pipeline can sequentially trigger each stage on every commit. Thus, the deployment step is triggered immediately after the Selenium test steps are complete. However, this would mean that the deployment from commit two overrides the last deployment in motion from commit one. The right approach is for commits two and three to wait for the deployment from commit one to complete, consolidate all the changes that have happened since commit one and trigger the deployment. If there is an issue, developers can easily figure out if the issue was introduced in commit two or commit three.

Jenkins Pipeline provides this functionality by enhancing the stage primitive. For example, a stage can have a concurrency level of one defined to indicate that at any point only one thread should be running through the stage. This achieves the desired state of running a deployment as fast as it should run.

[source, width="350"]
 stage name: 'Production', concurrency: 1
    node {
        unarchive mapping: ['target/x.war' : 'x.war']
        deploy 'target/x.war', 'production'
        echo 'Deployed to http://localhost:8888/production/'
    }

.Gates and Approvals

Continuous delivery means having binaries in a release ready state whereas continuous deployment means pushing the binaries to production - or automated deployments. Although
continuous deployment is a sexy term and a desired state, in reality organizations still want a human to give the final approval before bits are pushed to production.
This is captured through the “input” primitive in Pipeline. The input step can wait indefinitely for a human to intervene.

[source, width="350"]
input message: "Does http://localhost:8888/staging/ look good?"

.Deployment of Artifacts to Staging/Production

Deployment of binaries is the last mile in a pipeline. The numerous servers employed within the organization and available in the market make it difficult to employ a uniform deployment step. Today, these are solved by third-party deployer products whose job it is to focus on deployment of a particular stack to a data center. Teams can also write their own extensions to hook into the Pipeline job type and make the deployment easier.

Meanwhile, job creators can write a plain old Groovy function to define any custom steps that can deploy (or undeploy) artifacts from production.

[source, width="350"]
def deploy(war, id) {
    sh "cp ${war} /tmp/webapps/${id}.war"
}

.Restartable flows

All Pipelines are resumable, so if Jenkins needs to be restarted while a flow is running, it should resume at the same point in its execution after Jenkins starts back up. Similarly, if a flow is running a lengthy sh or bat step when a slave unexpectedly disconnects, no progress should be lost when connectivity is restored.

There are some cases when a flow build will have done a great deal of work and proceeded to a point where a transient error occurred: one which does not reflect the inputs to this build, such as source code changes. For example, after completing a lengthy build and test of a software component, final deployment to a server might fail because of network problems. 

Instead of needed to rebuild the entire flow, the Checkpoints Pipeline feature allows users to restart just the last portion of the flow, essentially running a new build with the tested artifacts of the last build. This feature is part of the CloudBees Jenkins Enterprise product.

[source, width="350"] 
node {
    sh './build-and-test'
}
checkpoint 'Completed tests'
node {
    sh './deploy'
}

.Pipeline Stage View

When you have complex builds pipelines, it is useful to see the progress of each stage and to see where build failures are occurring in the pipeline. This can enable users to debug which tests are failing at which stage or if there are other problems in their pipeline. Many organization also want to make their pipelines user-friendly for non-developers without having to develop a homegrown UI, which can prove to be a lengthy and ongoing development effort.

The Pipeline Stage View feature offers extended visualization of Pipeline build history on the index page of a flow project. This visualization also includes helpful metrics like average run time by stage and by build, and a user-friendly interface for interacting with input steps.

[[workflow-view]]
.CloudBees Pipeline View
image::ch13/workflow-big-responsive.png[scaledwidth=“90%”]

The only prerequisite for this plugin is a pipeline with defined stages in the flow. There can be as many stages as you desired and they can be in a linear sequence, and the stage names will be displayed as columns in the Stage View UI. This feature is part of the CloudBees Jenkins Enterprise product.


=== Artifact traceability with fingerprinting

Traceability is important for DevOps teams who need to be able to trace code from commit to deployment. It enables impact analysis by showing relationships between artifacts and allows for visibility into the full lifecycle of an artifact, from its code repository to where the artifact is eventually deployed in production.

Jenkins and the Pipeline feature support tracking versions of artifacts using file fingerprinting, which allows users to trace which downstream builds are using any given artifact. To fingerprint with Pipeline, simply add a “fingerprint: true” argument to any artifact archiving step. For example:

[source, width="350"]
step([$class: 'ArtifactArchiver', artifacts: '**/target/*.war’, fingerprint: true])

will archive any WAR artifacts created in the Pipeline and fingerprint them for traceability. This trace log of this artifact and a list of all fingerprinted artifacts in a build will then be available in the left-hand menu of Jenkins:

[[fingerprint-workflow]]
.List of fingerprinted files
image::ch13/workflow-fingerprint.png[scaledwidth=“90%”]

To find where an artifact is used and deployed to, simply follow the “more details” link through the artifact’s name and view the entires for the artifact in its “Usage” list.

[[fingerprinting]]
.Fingerprint of a WAR
image::ch13/fingerprinting.png[scaledwidth=“90%”]

For more information, visit the https://wiki.jenkins-ci.org/display/JENKINS/Fingerprint[Jenkins community’s wiki] on how fingerprints work and their usage.

=== Pipeline DSL Keywords

Pipelines are defined as scripts that tell Jenkins what to do within the pipeline, what tools to use, and which slave to use for the builds. These actions are pre-defined as steps and new steps can be created using plugins (see the “Plugin Compatibility” section for more). Below are some of the key steps in a Pipeline:

==== `node`: Allocate node

* This step schedules the tasks in its nested block on whatever node with the label specified in its argument. This step is required for all Pipelines so that Jenkins knows which slave should run the Pipeline steps. 
* Params:
  * `label`: String - label or name of a slave
  * Nested block
[source, width="350"]
node('master') {}

==== `stage`: Stage

* The “stage” command allows sections of the build to be constrained by a limited or unlimited concurrency, which is useful when preventing slow build stages like integration tests from overloading the build system.
* Params:
  * `name`: String, mandatory
  * `concurrency`: Integer that is greater than 0
[source, width="350"]
stage 'Dev'

====  `input`: Input

* This step allows
* Params:
  * `message` : String, default "Pipeline has paused and needs your input before proceeding"
  * `id`: Optional ID that uniquely identifies this input from all others.
  * `submitter`: Optional user/group name who can approve this.
  * `ok`: Optional caption of the OK button
  * `parameters`: List<ParameterDefinition>
[source, width="350"]
input ‘hello world’

====  `parallel`: Execute sequences of steps in parallel

* This step allows multiple actions to be performed at once, like integration tests or builds of multiple branches of a project.
* Params: Map, mandatory
[source, width="350"]
def branches = [:]
parallel branches



====  `bat`: Windows Batch Script

* This step allows a Jenkins server or slave running on a Windows machine to execute a batch script.
* Params:
  * `script`: String - the script to execute
[source, width="350"]
bat "${mvnHome}\\bin\\mvn -B verify"


====  `sh`: Shell Script

* This step allows a Jenkins server or slave running on Linux or a Unix-like machine to execute a shell script.
* Params:
  * `script`: String - the script to execute
[source, width="350"]
sh "${mvnHome}/bin/mvn -B verify"

More information on the Pipeline DSL can be found in https://github.com/jenkinsci/workflow-plugin/blob/master/TUTORIAL.md[the Pipeline tutorial].


=== Setting up a basic build Pipeline with Jenkins

Creating a pipeline in Jenkins with Pipeline is as simple as a basic script:

[source, width="350"]
node(‘linux’){
  git url: 'https://github.com/jglick/simple-maven-project-with-tests.git'
  def mvnHome = tool 'M3'
  env.PATH = "${mvnHome}/bin:${env.PATH}"
  sh 'mvn -B clean verify'
}

Where *node* is the step that schedules the tasks in the following block on whatever node with the label specified in its argument. In this case, the block’s tasks will only be run on a node with the label ‘linux’. The node block is required to tell Jenkins what system to run the commands. 

*git* is the step that specifies what source code repository code should be checked from and does the checkout at this point. 

The *tool* step makes sure a tool with the given name, in this case, a specific version of the Maven build tool, is installed on the current node. Merely running this step does not do much good; the script needs to know where it was installed, so the tool can be run later. For this, we need a variable.

The *def* keyword in Groovy is the quickest way to define a new variable, hence the “def mvnHome”. The return value of the *tool ‘M3’* check is assigned to the *mvnHome* variable.

Pipeline also allows for the creation of very complex pipelines, with parallel stages, conditional logic gates, and for definitions to be loaded from version control and shared between Pipelines. This allows for pipelines and certain standardized scripts to be shared between teams and changes to these scripts to be protected and reviewed by an administrator. 

Here is an example script for such a scenario, where the bulk of the script is version controlled in GitHub:

[source, width="350"]
def flow
node('master') {
    git branch: 'master', changelog: false, poll: true, url: 'https://github.com/lavaliere/workflow-plugin-pipeline-demo.git'
    flow = load 'flow.groovy'
    flow.devQAStaging()
}
flow.production()

And here is the example script’s GitHub counterpart:

[source, width="350"]
def devQAStaging() {
    env.PATH="${tool 'Maven 3.x'}/bin:${env.PATH}"
    stage 'Dev'
    sh 'mvn clean install package'
    archive 'target/x.war'
  try {
        checkpoint('Archived war')
    } catch (NoSuchMethodError _) {
        echo 'Checkpoint feature available in Jenkins Enterprise by CloudBees.'
    }
    stage 'QA'
    parallel(longerTests: {
        runWithServer {url ->
            sh "mvn -f sometests/pom.xml test -Durl=${url} -Dduration=30"
        }
    }, quickerTests: {
        runWithServer {url ->
            sh "mvn -f sometests/pom.xml test -Durl=${url} -Dduration=20"
        }
    })
    stage name: 'Staging', concurrency: 1
    deploy 'target/x.war', 'staging'
}
def production() {
    input message: "Does http://localhost:8888/staging/ look good?"
    try {
        checkpoint('Before production')
    } catch (NoSuchMethodError _) {
        echo 'Checkpoint feature available in Jenkins Enterprise by CloudBees.'
    }
    stage name: 'Production', concurrency: 1
    node {
        unarchive mapping: ['target/x.war' : 'x.war']
        deploy 'target/x.war', 'production'
        echo 'Deployed to http://localhost:8888/production/'
    }
}
def deploy(war, id) {
    sh "cp ${war} /tmp/webapps/${id}.war"
}
def undeploy(id) {
    sh "rm /tmp/webapps/${id}.war"
}
return this;

More information on complex Pipelines can be found http://jenkins-enterprise.cloudbees.com/docs/user-guide-docs/workflow.html[in the CloudBees Jenkins Enterprise documentation] and in the https://github.com/jenkinsci/workflow-plugin/blob/master/TUTORIAL.md[Pipeline repository].

=== Plugin compatibility

Many source plugins have already added support for the Pipeline feature, while others can be upgraded to support Pipeline through some code changes to utilize some newer APIs. 

====  Supported SCMs

- `GitSCM` (`git`): supported as of 2.3; native `git` step also bundled
- `SubversionSCM` (`subversion`): supported as of the Subversion Plugin’s v 2.5; native `svn` step also bundled
- `MercurialSCM` (`mercurial`): supported as of 1.51
- `PerforceScm` (`p4`, not the older `perforce`): supported as of 1.2.0


====  Build steps and post-build actions

- `ArtifactArchiver` (core)
- `Fingerprinter` (core)
- `JUnitResultArchiver` (`junit`)
- `JavadocArchiver` (`javadoc`)
- `Mailer` (`mailer`)
- `CopyArtifact` (`copyartifact`): https://issues.jenkins-ci.org/browse/JENKINS-24887[JENKINS-24887] in 1.34

====  Build wrappers

- API to integrate build wrappers: https://issues.jenkins-ci.org/browse/JENKINS-24673[JENKINS-24673]

====  Clouds

- `docker`: supported as of 0.8
- `nectar-vmware` (CloudBees Jenkins Enterprise): supported as of 4.3.2
- `ec2`: known to work as is

====  Miscellaneous

- `rebuild`: https://issues.jenkins-ci.org/browse/JENKINS-26024[JENKINS-26024]
- `build-token-root`: https://issues.jenkins-ci.org/browse/JENKINS-26693[JENKINS-26693]
- `credentials-binding`: `withCredentials` step as of 1.3
- `job-dsl`: implemented in 1.29


====  Custom steps

- `parallel-test-executor`: supported with `splitTests` step since 1.6
- `mailer`: https://issues.jenkins-ci.org/browse/JENKINS-26104[JENKINS-26104] in Pipeline 1.2

The most up to date list on available steps can always be found in the Pipeline snippet generator. The snippet generator is located at the bottom of the configuration page for every Pipeline job. Steps offered by a compatible plugin will appear in the snippet generator once the compatible plugin is installed.

There are some plugins which have not yet incorporated support for Pipeline. Guides on how to add this support, as well as their relevant Jenkins JIRA tickets can be found in the Pipeline Plugin https://github.com/jenkinsci/workflow-plugin/blob/master/COMPATIBILITY.md[GitHub documentation].