---
layout: simplepage
title: Hardware Recommendations
---
:notitle:
:description:
:author: R. Tyler Croy
:email: jenkinsci-users@googlegroups.com
:imagesdir: /doc/book/resources/hardware-recommendations
:sectanchors:
:toc: left

== Hardware Recommendations

=== Introduction

Sizing a Jenkins environment depends on a number of factors, which makes it a very inexact science. Achieving an optimal configuration requires some experience and experimentation. It is, however, possible to make a smart approximation to start - especially when designing with Jenkins’ best practices in mind.

The following outlines these factors and how you can account for them when sizing your configuration. You are also provided sample configurations and the hardwares behind some of the largest Jenkins installations presented in a Jenkins Scalability Summit.

=== Choosing the Right Hardware for Masters

One of the greatest challenges of properly setting up a Jenkins instance is that there is no _one size fits all_ answer - the exact specifications of the hardware that you will need will depend heavily on your organization's current and future needs.

Your Jenkins master will be serving HTTP requests and storing all of the important information for your Jenkins instance in its _$JENKINS_HOME_ folder (configurations, build histories and plugins).

*Note*: For masters that will run in a highly available configuration using the CloudBees High Availability plugin, the _$JENKINS_HOME_ will need to reside in a network-attached storage server, which then must be made accessible to the Jenkins servers using NFS or some other protocol.

More information on sizing masters based organizational needs can be found in http://jenkins-cookbook.cloudbees.com/docs/jenkins-cookbook/_right_sizing_jenkins_masters.html#_calculating_how_many_jobs_masters_and_executors_are_needed[Architecting for Scale].

==== Memory Requirements for the Master

The amount of memory Jenkins needs is largely dependent on many factors, which is why the RAM allotted for it can range from 200 MB for a small installation to 70+ GB for a single and massive Jenkins master. However, you should be able to estimate the RAM required based on your project build needs.

Each build node connection will take 2-3 threads, which equals about 2 MB or more of memory. You will also need to factor in CPU overhead for Jenkins if there are a lot of users who will be accessing the Jenkins user interface.

It is generally a bad practice to allocate executors on a master, as builds can quickly overload a master's CPU/memory/etc and crash the instance, causing unnecessary downtime. Instead, it is advisable to set up slaves that the Jenkins master can delegate build jobs to, keeping the bulk of the work off of the master itself.


=== Choosing the Right Slave Machines

The backbone of Jenkins is its ability to orchestrate builds, but installations which do not leverage Jenkins’ distributed builds architecture are artificially limiting the number of builds that their masters can orchestrate. More information on http://jenkins-cookbook.cloudbees.com/docs/jenkins-cookbook/_architecting_for_scale.html#_distributed_builds_architecture[the benefits of a distributed architecture] can be found in the Architecting for Scale section.

==== Requirements for a Machine to be a Slave

[[fungibility]]
Slave machines are typically generic x86 machines with enough memory to run specific build types. The slave machine’s configuration depends on the builds it will be used for and on the tools required by the same builds.

Configuring a machine to act as slave inside your infrastructure can be tedious and time consuming. This is especially true when the same set-up has to be replicated on a large pool of slaves. Because of this, is ideal to have fungible slaves, which are slaves that are easily replaceable. Slaves should be generic for all builds rather customized for a specific job or a set of jobs. The more generic the slaves, the more easily they are interchanged, which in turn allows for a better use of resources and a reduced impact on productivity if some slaves suffer an outage. Andrew Bayer introduced this concept of “fungibility” as applied to slaves during his presentation http://www.slideshare.net/andrewbayer/seven-habits-of-highly-effective-jenkins-users-2014-edition[“Seven Habits of Highly Effective Jenkins Users” at the Jenkins User Conference (Europe, 2014)].

The more automated the environment configuration is, the easier it is to replicate a configuration onto a new slave machine. Tools for configuration management or a pre-baked image can be excellent solutions to this end. Containers and virtualization are also popular tools for creating generic slave environments.

More information on estimating the number of executors needed in a given environment can be found in the http://jenkins-cookbook.cloudbees.com/docs/jenkins-cookbook/_architecting_for_scale.html[Architecting for Scale] section.

=== Sample use Cases Published Online

For help with regards to architecture and best practices, it is always a good idea to refer to real-life examples to learn from others’ experiences. All architecture decisions require some trade off, but you can save your organization time and money by anticipating and planning for these pains before you experience them.

In the Jenkins community, such examples can be found by presenters at the Jenkins User Conferences, scalability summits, and other such community events.

The following are case studies from the 2013 Jenkins Scalability Summit:

* Netflix
* Yahoo


==== Netflix

At the Scalability Summit, Gareth Bowles, a Sr. Build/Tool engineer at Netflix, explained how Netflix created its large installation and discussed some of the pain they encountered.

In 2012, Netflix had the following configuration:

* 1 master
- 700 engineers on one master
* Elastic slaves with Amazon EC2 + 40 ad-hoc slaves in Netflix’s data center
* 1,600 jobs
- 2,000 builds per day
- 2 TB of builds
- 15% build failures
* Hardware Configuration
- 2x quad core x86_64 for the master
- 26G memory
- m1.xlarge slave machines on AWS


[[netflix]]
.Netflix’s physical architecture
image::netflix-arch.png[scaledwidth=90%]


By 2013, Netflix split their monolithic master and significantly increased their number of jobs and builds:

* 6 masters
- 700 engineers on each master
* 100 slaves
- 250 executors, but generally 4 executors per slave
* 3,200 jobs
- 3 TB of builds
- 15% build failures

*Scheduling and Monitoring*

Netflix was able to spin up dynamic slaves using http://www.slideshare.net/bmoyles/the-dynaslave-plugin[the Dynaslave plugin], and configure Jenkins to automatically disable any jobs that hadn’t run successfully in 30 days. They were also able to monitor their slaves’ uptime using a Groovy script in a Jenkins job and which emailed the slaves’ teams if there was downtime. Furthermore, this script would also monitor slave connections for errors and would remove the offending slaves. Removed slaves were then de-provisioned.

==== Yahoo

Mujibur Wahab of Yahoo also presented Yahoo’s massive installation to the 2013 Scalability Summit. Their installation was:

* 1 primary master
- 1,000 engineers rely on this Jenkins instance
- 3 backup masters
- _$JENKINS_HOME_ lives on NetApp
* 50 Jenkins slaves in 3 data centers
- 400+ executors
* 13,000 jobs
- 8,000 builds/day
- 20% build failure rate
- 2 million builds/year and on target for 1 million/quarter
* Hardware Configuration
- 2 x Xeon E5645 2.40GHz, 4.80GT QPI (HT enabled, 12 cores, 24 threads)
- 96G memory
- 1.2TB disk
- 48GB max heap to JVM
- 20TB Filer volume to store Jenkins job and build data
  - This volume stores 6TB of build data

Here is an overview of their architecture, as taken from Wahab’s slides:

[[yahoo]]
.Yahoo’s Physical Architecture
image::yahoo-architecture.png[scaledwidth=90%]

Because continuous delivery is so critical to Yahoo, they created a Jenkins team to develop tools related to their pipeline and provide Jenkins-as-a-service to the internal Yahoo teams. The Jenkins team is not responsible for job configurations or creating the pipelines, just the uptime of the infrastructure. The health of their infrastructure is monitored by other existing mechanisms.

Yahoo quickly found that running only one build per slave was a problem because it would be impossible to continue adding new hardware to scale with their increasing build needs.
To solve this, they started using an LXC-like chroot scheme to emulate virtualization.
This light-weight container is a heavily-augmented version of the standard UNIX command _chroot_.
Their version installs all files need to create a functional, clean software environment and provides the ability to manage those virtual environments.
Each VM gets two threads and  4GB of memory to accommodate their Maven builds.
